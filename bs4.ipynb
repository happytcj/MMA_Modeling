{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scraperwiki\n",
    "from string import ascii_lowercase\n",
    "import urllib2\n",
    "import urllib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://www.fightmetric.com/statistics/fighters'\n",
    "html = scraperwiki.scrape(url)\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_fighter = soup.find_all('tbody')[0]\n",
    "rows_fighter = table_fighter.find_all('tr')[1:]\n",
    "# for i in rows_fighter[0]:\n",
    "#     print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_career_stats(url):\n",
    "    ret = []\n",
    "    html = scraperwiki.scrape(url)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    div_stats = soup.find_all('div', {'class': 'b-list__info-box-left clearfix'})[0]\n",
    "    list_items_stats = div_stats.find_all('li', {'class': 'b-list__box-list-item b-list__box-list-item_type_block'})\n",
    "    for i in list_items_stats:\n",
    "        txt = i.text.strip()\n",
    "        txt = ' '.join(txt.split()).split(': ')[-1]\n",
    "        if len(txt) > 0:\n",
    "#             print (txt)\n",
    "            ret.append(txt)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1.35', u'30%', u'3.55', u'38%', u'1.07', u'33%', u'66%', u'0.0']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_url = 'http://www.fightmetric.com/fighter-details/b361180739bed4b0'\n",
    "get_career_stats(test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loop through a single fighter row and capture all the information\n",
    "def parse_row(row):\n",
    "    ret = []\n",
    "    career_stats = []\n",
    "    \n",
    "    cols_fighter = row.find_all('td')\n",
    "    for idx, col in enumerate(cols_fighter):\n",
    "        links = col.find_all('a')\n",
    "        num_links = len(links)\n",
    "        if num_links == 1:\n",
    "            link = links[0]\n",
    "            try:\n",
    "                ret.append(link.contents[0])\n",
    "                if idx == 0:\n",
    "                    career_stats = get_career_stats(link['href'])\n",
    "            except IndexError as e:\n",
    "                ret.append('n/a')\n",
    "        elif num_links > 1:\n",
    "            raise Exception('invalid link parsing')\n",
    "        else:\n",
    "    #         print repr(col.contents[0])\n",
    "            txt = col.contents[0].strip()\n",
    "            if txt != '':\n",
    "                if '-' in txt:\n",
    "                    ret.append('n/a')\n",
    "                else:\n",
    "                    ret.append(col.contents[0].strip())\n",
    "            else:\n",
    "                ret.append('n/a')\n",
    "    ret = ret + career_stats\n",
    "#     print ret\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = ['first', 'last', 'nickname', 'height', 'weight', 'reach', \n",
    "          'stance', 'w', 'l', 'd', 'belt', 'SLpM', 'Str. Acc.', 'SApM',\n",
    "         'Str. Def', 'TD Avg', 'TD Acc.', 'TD Def.', 'Sub. Avg.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for c in ascii_lowercase:\n",
    "    url = 'http://www.fightmetric.com/statistics/fighters?char=%c&page=all' %c\n",
    "    html = scraperwiki.scrape(url)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    table_fighter = soup.find_all('tbody')[0]\n",
    "    rows_fighter = table_fighter.find_all('tr')[1:]\n",
    "    for row in rows_fighter:\n",
    "        res.append(parse_row(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res, columns=header)\n",
    "df.to_csv('tmp_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fighters = []\n",
    "for table in tables:\n",
    "    wcname = str(table.find('div', {'class': 'weight-class-name'}).contents[0]).strip()\n",
    "    if 'Pound-for-Pound' in wcname:\n",
    "        continue\n",
    "    else:\n",
    "#         print table.a.contents[0]\n",
    "        fighters.append(table.a.contents[0])\n",
    "        other_ranked = table.find_all('td', {'class': 'name-column'})\n",
    "        for ranked in other_ranked:\n",
    "            print ranked.a.contents[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = []\n",
    "for table in tables:\n",
    "    all_td = table.find_all('a')\n",
    "    all_td_str = set([x.string for x in all_td if x.string is not None])\n",
    "    all_td_str = [x.encode('utf-8').strip() for x in all_td_str if 'County' not in x]\n",
    "    all_td_str = [x for x in all_td_str if 'District' not in x and 'spade' not in x and 'Piercer' not in x and '\\xbc' not in x and 'Generals' not in x and 'Marquis' not in x]\n",
    "    all_td_str = [x for x in all_td_str if 'home' not in x and 'Yama' not in x]\n",
    "    all_td_str = [x for x in all_td_str if 'Spade' not in x]\n",
    "    all_td_str = [x for x in all_td_str if ' ' in x]\n",
    "    all_td_str = [[x] for x in all_td_str if len(x.split(' ')) < 4]\n",
    "    names += all_td_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./chinese_names.csv', 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for year in range(1880, 1930):\n",
    "    post_params = {\n",
    "    'top' : '100',\n",
    "    'year' : str(year)\n",
    "    }\n",
    "    post_args = urllib.urlencode(post_params)\n",
    "    urllib2.urlopen(url, post_args)\n",
    "    headers = {\n",
    "        'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language' : 'en-US,en;q=0.5',\n",
    "        'Connection' : 'keep-alive',\n",
    "        'Host' : 'www.ssa.gov',\n",
    "        'Referer' : 'http://www.ssa.gov/cgi-bin/popularnames.cgi',\n",
    "        'User-Agent' : 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0'\n",
    "        }\n",
    "\n",
    "    # With POST data:\n",
    "    page = urllib.urlopen(url, post_args, headers)\n",
    "    soup = BeautifulSoup(page.read(), 'lxml')\n",
    "    rank = soup.find('table', {'border': '1', 'bordercolor' : '#aaaabb'})\n",
    "    all_td = rank.find_all('td')\n",
    "    all_td_str = [x.string for x in all_td]\n",
    "    all_td_str.pop(-1)\n",
    "    B = np.reshape(all_td_str, (-1, 3))\n",
    "    B = [list(x)[1:] for x in B]\n",
    "    \n",
    "    with open('./names/'+str(year)+'_names.csv', 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerows(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
