{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scraperwiki\n",
    "from string import ascii_lowercase\n",
    "import urllib2\n",
    "import urllib\n",
    "import time\n",
    "from selenium   import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.fightmetric.com/statistics/fighters'\n",
    "html = scraperwiki.scrape(url)\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_fighter = soup.find_all('tbody')[0]\n",
    "rows_fighter = table_fighter.find_all('tr')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_career_stats(url):\n",
    "    ret = []\n",
    "    html = scraperwiki.scrape(url)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    div_stats = soup.find_all('div', {'class': 'b-list__info-box-left clearfix'})[0]\n",
    "    list_items_stats = div_stats.find_all('li', {'class': 'b-list__box-list-item b-list__box-list-item_type_block'})\n",
    "    for i in list_items_stats:\n",
    "        txt = i.text.strip()\n",
    "        txt = ' '.join(txt.split()).split(': ')[-1]\n",
    "        if len(txt) > 0:\n",
    "#             print (txt)\n",
    "            ret.append(txt)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1.35', u'30%', u'3.55', u'38%', u'1.07', u'33%', u'66%', u'0.0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_url = 'http://www.fightmetric.com/fighter-details/b361180739bed4b0'\n",
    "get_career_stats(test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through a single fighter row and capture all the information\n",
    "def parse_row(row):\n",
    "    ret = []\n",
    "    career_stats = []\n",
    "    \n",
    "    cols_fighter = row.find_all('td')\n",
    "    for idx, col in enumerate(cols_fighter):\n",
    "        links = col.find_all('a')\n",
    "        num_links = len(links)\n",
    "        if num_links == 1:\n",
    "            link = links[0]\n",
    "            try:\n",
    "                ret.append(link.contents[0])\n",
    "                if idx == 0:\n",
    "                    career_stats = get_career_stats(link['href'])\n",
    "            except IndexError as e:\n",
    "                ret.append('n/a')\n",
    "        elif num_links > 1:\n",
    "            raise Exception('invalid link parsing')\n",
    "        else:\n",
    "    #         print repr(col.contents[0])\n",
    "            txt = col.contents[0].strip()\n",
    "            if txt != '':\n",
    "                if '-' in txt:\n",
    "                    ret.append('n/a')\n",
    "                else:\n",
    "                    ret.append(col.contents[0].strip())\n",
    "            else:\n",
    "                ret.append('n/a')\n",
    "    ret = ret + career_stats\n",
    "#     print ret\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['first', 'last', 'nickname', 'height', 'weight', 'reach', \n",
    "          'stance', 'w', 'l', 'd', 'belt', 'SLpM', 'Str. Acc.', 'SApM',\n",
    "         'Str. Def', 'TD Avg', 'TD Acc.', 'TD Def.', 'Sub. Avg.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for c in ascii_lowercase[15:16]:\n",
    "    url = 'http://www.fightmetric.com/statistics/fighters?char=%c' %c\n",
    "#     print url\n",
    "#     break\n",
    "    html = scraperwiki.scrape(url)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    table_fighter = soup.find_all('tbody')[0]\n",
    "    rows_fighter = table_fighter.find_all('tr')[1:]\n",
    "    for row in rows_fighter:\n",
    "        try:\n",
    "            res.append(parse_row(row))\n",
    "        except:\n",
    "            print 'there was an issue with this url: %s' %url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res, columns=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull all of Bisping's fights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull all of Bisping's fights\n",
    "url = 'http://www.fightmetric.com/fighter-details/2b93ebd9f5417ad2'\n",
    "html = scraperwiki.scrape(url)\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_fighter = soup.find_all('tbody')[0]\n",
    "rows_fighter = table_fighter.find_all('a', {'class': 'b-link b-link_style_black'})\n",
    "rows_fighter = [x for x in rows_fighter if 'fighter' in x['href'] and\n",
    "                'Bisping' not in x.text.strip()]\n",
    "\n",
    "rows_results = table_fighter.find_all('i', {'class': 'b-flag__text'})\n",
    "\n",
    "res = [['name', 'result']]\n",
    "for idx, row in enumerate(rows_fighter):\n",
    "    fighter = rows_fighter[idx].text.strip()\n",
    "    result = rows_results[idx].text.strip()\n",
    "    res.append([fighter, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_np = np.asarray(res)\n",
    "np.savetxt('bisping_results.csv', res_np, delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pull all fights of top 15 fighters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 fighters in the top 15 of each division\n"
     ]
    }
   ],
   "source": [
    "fighters = np.genfromtxt('data/top15.csv', delimiter=',', dtype=str)\n",
    "print '%d fighters in the top 15 of each division' %len(fighters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = webdriver.Chrome('/usr/local/lib/chromedriver')\n",
    "d.implicitly_wait(10) #seconds\n",
    "d.get('http://www.fightmetric.com/statistics/fighters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for fighter in fighters:\n",
    "    first = fighter.split(' ')[0]\n",
    "    last = fighter.split(' ')[-1]\n",
    "    e = d.find_element_by_class_name('b-statistics__search-field')\n",
    "    e.clear()\n",
    "    e.send_keys(last)\n",
    "    e.send_keys(Keys.RETURN)\n",
    "    url = d.current_url\n",
    "    html = scraperwiki.scrape(url)\n",
    "    query_soup = BeautifulSoup(html, 'lxml')\n",
    "    query_results = query_soup.find_all('a', attrs={'class':'b-link b-link_style_black'})\n",
    "    for one_query_result in query_results:\n",
    "        if first in one_query_result.getText():\n",
    "    #         d.get(one_query_result['href'])\n",
    "            html = scraperwiki.scrape(one_query_result['href'])\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            table_fighter = soup.find_all('tbody')[0]\n",
    "            rows_fighter = table_fighter.find_all('a', {'class': 'b-link b-link_style_black'})\n",
    "            rows_fighter = [x for x in rows_fighter if 'fighter' in x['href'] and\n",
    "                            last not in x.text.strip()]\n",
    "\n",
    "            rows_results = table_fighter.find_all('i', {'class': 'b-flag__text'})\n",
    "            rows_results = [x for x in rows_results if 'next' not in x.text.strip()]\n",
    "            for idx, row in enumerate(rows_fighter):\n",
    "                opponent = rows_fighter[idx].text.strip()\n",
    "                result = rows_results[idx].text.strip()\n",
    "                res.append([fighter, opponent, result])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./top15_results.csv', res, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pull all fights of all fighters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: should I pull data for more fighters? I wonder if this will improve error rate\n",
    "raw_data = pd.read_csv('data/fighter_data.csv')\n",
    "df_5w = raw_data[((raw_data['w'].astype(int)+raw_data['l'].astype(float)+raw_data['d'].astype(float)) >= 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = webdriver.Chrome('/usr/local/lib/chromedriver')\n",
    "d.implicitly_wait(10) #seconds\n",
    "d.get('http://www.fightmetric.com/statistics/fighters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fighters = df_5w[['first', 'last']]\n",
    "df_fighters.loc[:, 'fighters'] = df_fighters['first'] + ' ' + df_fighters['last']\n",
    "fighters = df_fighters['fighters'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for fighter in fighters:\n",
    "    first = fighter.split(' ')[0]\n",
    "    last = fighter.split(' ')[-1]\n",
    "    e = d.find_element_by_class_name('b-statistics__search-field')\n",
    "    e.clear()\n",
    "    e.send_keys(last)\n",
    "    e.send_keys(Keys.RETURN)\n",
    "    url = d.current_url\n",
    "    html = scraperwiki.scrape(url)\n",
    "    query_soup = BeautifulSoup(html, 'lxml')\n",
    "    query_results = query_soup.find_all('a', attrs={'class':'b-link b-link_style_black'})\n",
    "    for one_query_result in query_results:\n",
    "        if first in one_query_result.getText():\n",
    "            try:\n",
    "                html = scraperwiki.scrape(one_query_result['href'])\n",
    "                soup = BeautifulSoup(html, 'lxml')\n",
    "                table_fighter = soup.find_all('tbody')[0]\n",
    "                rows_fighter = table_fighter.find_all('a', {'class': 'b-link b-link_style_black'})\n",
    "                rows_fighter = [x for x in rows_fighter if 'fighter' in x['href'] and\n",
    "                                last not in x.text.strip()]\n",
    "\n",
    "                rows_results = table_fighter.find_all('i', {'class': 'b-flag__text'})\n",
    "                rows_results = [x for x in rows_results if 'next' not in x.text.strip()]\n",
    "                for idx, row in enumerate(rows_fighter):\n",
    "                    opponent = rows_fighter[idx].text.strip()\n",
    "                    result = rows_results[idx].text.strip()\n",
    "                    res.append([fighter, opponent, result])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./full_results.csv', res, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
